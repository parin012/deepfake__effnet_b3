{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650248fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import amp\n",
    "\n",
    "from configs.config import (\n",
    "    OUT_DIR, DEVICE, EPOCHS, WARMUP_EPOCHS, PATIENCE,\n",
    "    USE_TTA_HFLIP, INIT_WEIGHTS, INIT_META,\n",
    "    LR_BACKBONE, LR_HEAD, SEED, IMG_SIZE\n",
    ")\n",
    "from data.dataset import create_dataloaders\n",
    "from models.efficientnet_b3 import build_model, FocalLoss, ModelEMA\n",
    "from train.utils import (\n",
    "    set_seed, find_best_threshold,\n",
    "    save_best_model, save_last\n",
    ")\n",
    "\n",
    "\n",
    "def tta_logits(model, x):\n",
    "    logits = model(x)\n",
    "    if USE_TTA_HFLIP:\n",
    "        logits = 0.5 * (logits + model(torch.flip(x, dims=[3])))\n",
    "    return logits\n",
    "\n",
    "\n",
    "def load_init_weights(model):\n",
    "    if INIT_WEIGHTS and os.path.exists(INIT_WEIGHTS):\n",
    "        print(f\"[init] loading weights from {INIT_WEIGHTS}\")\n",
    "        sd = torch.load(INIT_WEIGHTS, map_location=\"cpu\")\n",
    "        if isinstance(sd, dict) and \"model\" in sd:\n",
    "            sd = sd[\"model\"]\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "        if missing:\n",
    "            print(f\"[init] missing keys({len(missing)}): {missing[:5]}\")\n",
    "        if unexpected:\n",
    "            print(f\"[init] unexpected keys({len(unexpected)}): {unexpected[:5]}\")\n",
    "\n",
    "\n",
    "def build_optimizer_and_scheduler(model):\n",
    "    head = model.get_classifier() if hasattr(model, \"get_classifier\") else list(model.children())[-1]\n",
    "    head_params = list(head.parameters()) if hasattr(head, \"parameters\") else []\n",
    "    head_param_ids = {id(p) for p in head_params}\n",
    "    backbone_params = [p for p in model.parameters() if id(p) not in head_param_ids]\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": backbone_params, \"lr\": LR_BACKBONE},\n",
    "            {\"params\": head_params, \"lr\": LR_HEAD},\n",
    "        ],\n",
    "        weight_decay=1e-4,\n",
    "    )\n",
    "\n",
    "    MIN_LR_FACTOR = 0.05\n",
    "\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            return float(epoch + 1) / float(max(1, WARMUP_EPOCHS))\n",
    "        progress = (epoch - WARMUP_EPOCHS) / float(max(1, EPOCHS - WARMUP_EPOCHS))\n",
    "        cosine = 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "        return MIN_LR_FACTOR + (1 - MIN_LR_FACTOR) * cosine\n",
    "\n",
    "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_lambda)\n",
    "    return opt, sch\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # ---------- Data ----------\n",
    "    train_loader, val_loader, train_ds, val_ds = create_dataloaders()\n",
    "    n_real = sum(lbl == 0 for _, lbl in train_ds.samples)\n",
    "    n_fake = sum(lbl == 1 for _, lbl in train_ds.samples)\n",
    "    pos_ratio = n_fake / max(1, n_real + n_fake)\n",
    "    alpha = 1 - pos_ratio\n",
    "    print(f\"[DATA] Train: Real={n_real}, Fake={n_fake}, pos_ratio={pos_ratio:.3f}, alpha={alpha:.3f}\")\n",
    "\n",
    "    # ---------- Model / Loss / EMA ----------\n",
    "    model = build_model().to(DEVICE).to(memory_format=torch.channels_last)\n",
    "    load_init_weights(model)\n",
    "\n",
    "    ema = ModelEMA(model)\n",
    "    crit = FocalLoss(alpha=alpha)\n",
    "    crit_val = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    opt, sch = build_optimizer_and_scheduler(model)\n",
    "\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    scaler = amp.GradScaler(enabled=(DEVICE == \"cuda\" and amp_dtype == torch.float16))\n",
    "\n",
    "    # ---------- Metrics logging ----------\n",
    "    metrics_path = os.path.join(OUT_DIR, \"metrics.csv\")\n",
    "    if os.path.exists(metrics_path):\n",
    "        os.remove(metrics_path)\n",
    "    pd.DataFrame(\n",
    "        columns=[\"epoch\", \"train_loss\", \"val_loss\", \"val_macroF1\", \"thr\", \"lr\"]\n",
    "    ).to_csv(metrics_path, index=False)\n",
    "\n",
    "    best_f1, best_thr, best_ep = -1.0, 0.5, -1\n",
    "    if INIT_META and os.path.exists(INIT_META):\n",
    "        try:\n",
    "            meta0 = json.load(open(INIT_META))\n",
    "            best_f1 = float(meta0.get(\"macro_f1\", best_f1))\n",
    "            best_thr = float(meta0.get(\"best_thr\", best_thr))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # ---------- Train loop ----------\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        tr_loss, n_tr = 0.0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"train[{ep}/{EPOCHS}]\")\n",
    "\n",
    "        for x, y in pbar:\n",
    "            x = x.to(DEVICE, non_blocking=True, memory_format=torch.channels_last)\n",
    "            y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(DEVICE == \"cuda\")):\n",
    "                logits = model(x)\n",
    "                loss = crit(logits, y)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            ema.update(model)\n",
    "\n",
    "            bs = x.size(0)\n",
    "            tr_loss += loss.item() * bs\n",
    "            n_tr += bs\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "        tr_loss /= max(1, n_tr)\n",
    "\n",
    "        # ---------- Validation (EMA + TTA) ----------\n",
    "        model.eval()\n",
    "        ema_model = ema.ema\n",
    "\n",
    "        val_loss, n_val = 0.0, 0\n",
    "        probs, ys = [], []\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(DEVICE, non_blocking=True, memory_format=torch.channels_last)\n",
    "                y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "                with amp.autocast(\"cuda\", dtype=amp_dtype, enabled=(DEVICE == \"cuda\")):\n",
    "                    logits = tta_logits(ema_model, x)\n",
    "                    v = crit_val(logits, y)\n",
    "\n",
    "                val_loss += float(v.item()) * x.size(0)\n",
    "                n_val += x.size(0)\n",
    "\n",
    "                p = torch.sigmoid(logits).squeeze(1).float().cpu().numpy()\n",
    "                probs.extend(p.tolist())\n",
    "                ys.extend(y.squeeze(1).cpu().numpy().astype(int).tolist())\n",
    "\n",
    "        val_loss /= max(1, n_val)\n",
    "        probs = np.array(probs)\n",
    "        ys = np.array(ys)\n",
    "\n",
    "        thr, f1 = find_best_threshold(probs, ys)\n",
    "        lr_now = float(opt.param_groups[0][\"lr\"])\n",
    "\n",
    "        pd.DataFrame([{\n",
    "            \"epoch\": ep,\n",
    "            \"train_loss\": tr_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_macroF1\": f1,\n",
    "            \"thr\": thr,\n",
    "            \"lr\": lr_now,\n",
    "        }]).to_csv(metrics_path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        sch.step()\n",
    "\n",
    "        improved = \"\"\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr, best_ep = f1, thr, ep\n",
    "            meta = {\n",
    "                \"best_thr\": float(best_thr),\n",
    "                \"macro_f1\": float(best_f1),\n",
    "                \"epoch\": ep,\n",
    "                \"alpha\": float(alpha),\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"img_size\": IMG_SIZE,\n",
    "                \"mean\": [0.485, 0.456, 0.406],\n",
    "                \"std\": [0.229, 0.224, 0.225],\n",
    "            }\n",
    "            save_best_model(ema_model, meta, OUT_DIR)\n",
    "            epochs_no_improve = 0\n",
    "            improved = \"✅\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(\n",
    "            f\"{improved} EP{ep:02d} | train_loss={tr_loss:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} | val_macroF1={f1:.4f} (thr={thr:.3f}) \"\n",
    "            f\"| best={best_f1:.4f}@{best_ep} | lr={lr_now:.2e}\"\n",
    "        )\n",
    "\n",
    "        save_last(model, ema_model, opt, ep, global_step, OUT_DIR)\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"⛔ EarlyStopping: {PATIENCE} epochs no improvement (best F1={best_f1:.4f}@{best_ep})\")\n",
    "            break\n",
    "\n",
    "    df = pd.read_csv(metrics_path)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"val\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(df[\"val_macroF1\"], label=\"F1\")\n",
    "    if best_ep != -1:\n",
    "        plt.axhline(best_f1, linestyle=\"--\", label=f\"best={best_f1:.4f}@{best_ep}\")\n",
    "    plt.ylabel(\"Macro-F1\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(df[\"thr\"], label=\"thr\")\n",
    "    plt.plot(df[\"lr\"], label=\"lr\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"thr / lr\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"curves.png\"), dpi=150)\n",
    "    print(\"[done] training finished, curves saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
